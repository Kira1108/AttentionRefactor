{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, \n",
    "    LSTM, \n",
    "    Embedding, \n",
    "    Dense, \n",
    "    Bidirectional,\n",
    "    Softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder passed test\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, T, V, D, M, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = Embedding(V, D, input_length = T)\n",
    "        self.lstm = Bidirectional(LSTM(M, return_sequences = True))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "def test_encoder(T = 10, V = 2000, D = 200, M = 15, N = 128):\n",
    "    data = np.random.random((N, T))\n",
    "    encoder = Encoder(T, V, D, M)\n",
    "    assert encoder(data).shape == (N, T, 2 * M)\n",
    "    print(\"Encoder passed test\")\n",
    "\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax over the second dimension(Not the last one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 10\n",
    "# V = 2000\n",
    "# D = 200\n",
    "# M = 15\n",
    "# N = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed softmax test\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax(axis = 1) # softmax over time\n",
    "\n",
    "def test_softmaxovertime():\n",
    "    data = np.random.random((128,20,1))\n",
    "    val = softmax(data)[np.random.randint(0,20)].numpy().sum()\n",
    "    assert abs(val -1) <= 1e-4\n",
    "    print('Passed softmax test')\n",
    "    \n",
    "test_softmaxovertime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the decoder header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder header(embedding) test success.\n"
     ]
    }
   ],
   "source": [
    "class DecoderHeader(tf.keras.layers.Layer):\n",
    "    def __init__(self, V, D, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        T = 1\n",
    "        self.embedding = Embedding(V, D, input_length = T)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        return x\n",
    "\n",
    "def test_decoder_header(V = 2000, D = 200, N = 128):\n",
    "    decoder_header = DecoderHeader(V, D)\n",
    "    decoder_input_data = np.random.randint(0,100, (N, 1))\n",
    "    assert decoder_header(decoder_input_data).shape == (N, 1, D)\n",
    "    print(\"Decoder header(embedding) test success.\")\n",
    "\n",
    "test_decoder_header()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder lstm test successfully\n"
     ]
    }
   ],
   "source": [
    "class DecoderLSTM(tf.keras.layers.Layer):\n",
    "    def __init__(self, M, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.M = M\n",
    "        self.lstm = LSTM(M, return_state = True)\n",
    "\n",
    "    def call(self, inputs, state = None):\n",
    "        if not state:\n",
    "            s = tf.zeros((len(inputs),self.M))\n",
    "            state = [s, s]\n",
    "        return self.lstm(inputs, initial_state = state)\n",
    "\n",
    "def test_decoder(N = 128, D = 200, M = 15):\n",
    "    decoder_lstm = DecoderLSTM(M)\n",
    "\n",
    "    o, h, c = decoder_lstm(\n",
    "        np.random.random((N, 1, D)), \n",
    "        state = [tf.zeros((N,M)),\n",
    "        tf.zeros((N,M))]\n",
    "        )\n",
    "\n",
    "    assert  all((o == h).numpy().flatten())\n",
    "    assert o.shape == h.shape == c.shape ==  (N,M)\n",
    "    print(\"Decoder lstm test successfully\")\n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the 2 decoder components inside 1 decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, V, D, M, T = 1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.M = M\n",
    "        self.embedding = Embedding(V, D, input_length = T)\n",
    "        self.lstm = LSTM(M, return_state = True)\n",
    "        self.dense = Dense(V, activation = 'softmax')\n",
    "\n",
    "    def call(self, inputs, state = None):\n",
    "        if not state:\n",
    "            s = tf.zeros((len(inputs),self.M))\n",
    "            state = [s, s]\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        o, h, c = self.lstm(x, initial_state = state)\n",
    "        output = self.dense(o)\n",
    "        return output, [h,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed decoder test\n"
     ]
    }
   ],
   "source": [
    "def test_decoder(V = 2000, M = 15,D = 200,N = 128,T = 1):\n",
    "    s = tf.zeros((N, M))\n",
    "    states = [[s,s],None]\n",
    "    for state in states:\n",
    "    \n",
    "        decoder = Decoder(V, D, M ,T)\n",
    "        out, state = decoder(np.random.randint(1,V, (N,T)), state = None)\n",
    "        assert out.shape == (N, V)\n",
    "        assert state[0].shape == (N, M)\n",
    "        assert state[1].shape == (N, M)\n",
    "    print('Passed decoder test')\n",
    "\n",
    "test_decoder()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The context vector is ignored here, so I need rebuild the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, V, D, M, T = 1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.M = M\n",
    "        self.embedding = Embedding(V, D, input_length = T)\n",
    "        self.lstm = LSTM(M, return_state = True)\n",
    "        self.dense = Dense(V, activation = 'softmax')\n",
    "        self.concat = Concatenate(axis = -1)\n",
    "\n",
    "    def call(self, inputs, state = None):\n",
    "        \n",
    "        if not state:\n",
    "            s = tf.zeros((len(inputs[0]),self.M))\n",
    "            state = [s, s]\n",
    "\n",
    "        # input contains 2 things\n",
    "        # 1 - decoder sequence input\n",
    "        # 2 - context vectors input\n",
    "        # these 2 things a detached here\n",
    "        decoder_inputs, context = inputs\n",
    "        \n",
    "        # embedding decoder sequence inputs\n",
    "        x = self.embedding(decoder_inputs)\n",
    "\n",
    "        # concat context and embedded vectors\n",
    "        x = self.concat([x, context])\n",
    "\n",
    "        # go through decoder lstm\n",
    "        o, h, c = self.lstm(x, initial_state = state)\n",
    "\n",
    "        # go through decoder dense\n",
    "        output = self.dense(o)\n",
    "\n",
    "        # return decoder final output and lstm states\n",
    "        return output, [h,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passes all decoder test\n"
     ]
    }
   ],
   "source": [
    "def decoder_full_test(N = 128,V = 2000,D = 200,M = 15,T = 1):\n",
    "\n",
    "    decoder = Decoder(V, D, M ,T)\n",
    "    decoder_in_data = np.random.randint(1,V, (N,T))\n",
    "    context = np.random.random((N, T, M))\n",
    "    decoder_inputs = [decoder_in_data, context]\n",
    "    out, state = decoder(decoder_inputs, state = None)\n",
    "    assert out.shape == (N, V)\n",
    "    assert state[0].shape == (N, M) == state[1].shape\n",
    "    print(\"Passes all decoder test\")\n",
    "decoder_full_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import RepeatVector, Concatenate, Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention test success\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, T, M, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.M = M\n",
    "        self.repeator = RepeatVector(T)\n",
    "        self.concater = Concatenate()\n",
    "        self.densor1 = Dense(M)\n",
    "        self.densor2 = Dense(1)\n",
    "        self.dotter = Dot(axes = 1)\n",
    "        self.softmax = Softmax(axis = 1)\n",
    "        \n",
    "    def call(self, encoder_output, prev_decoder_state):\n",
    "        s_repeated = self.repeator(prev_decoder_state)\n",
    "        concat_vector = self.concater([encoder_output, s_repeated])\n",
    "        weights = self.densor1(concat_vector)\n",
    "        weights = self.densor2(weights)\n",
    "        weights = self.softmax(weights)\n",
    "        context = self.dotter([weights, encoder_output])\n",
    "        return context\n",
    "    \n",
    "def test_attention(T = 100, M = 15, N = 128):\n",
    "\n",
    "    prev_state = tf.zeros((N, M))\n",
    "    encoder_out_tensor = tf.random.normal((N, T, 2*M))\n",
    "    context = AttentionLayer(T, M)(encoder_out_tensor,prev_state)\n",
    "    assert context.shape == (N, 1, 2*M)\n",
    "    print(\"Attention test success\")\n",
    "    \n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel(tf.keras.models.Model):\n",
    "    \n",
    "    def __init__(self, T, D, V, M, Ty = 50, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.M = M\n",
    "        self.Ty = Ty\n",
    "        self.encoder = Encoder(T,V,D,M)\n",
    "        self.attention = AttentionLayer(T, M)\n",
    "        self.decoder = Decoder(V, D, M)\n",
    "        \n",
    "    def call(self, encoder_input, decoder_input, state = None):\n",
    "        \n",
    "        if not state:\n",
    "            zero = tf.zeros((len(encoder_input),self.M))\n",
    "            state = [zero, zero]\n",
    "            \n",
    "        s, c = state\n",
    "        \n",
    "        encoder_output_tensor = self.encoder(encoder_input)\n",
    "        \n",
    "        out_tensors = []\n",
    "        for t in range(self.Ty):\n",
    "            context = self.attention(encoder_output_tensor, s)\n",
    "            out, state = self.decoder([decoder_input[:, t:t+1], context],[s, c])\n",
    "            out_tensors.append(out)\n",
    "            \n",
    "        out_tensors = tf.stack(out_tensors)\n",
    "        return tf.transpose(out_tensors, perm = [1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "T = 100\n",
    "D = 200\n",
    "V = 2000\n",
    "M = 15\n",
    "Ty = 50\n",
    "\n",
    "final_model = TrainModel(T, D, V, M, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 50, 2000), dtype=float32, numpy=\n",
       "array([[[0.00050004, 0.00050038, 0.00049998, ..., 0.00050082,\n",
       "         0.00049933, 0.0004999 ],\n",
       "        [0.00050039, 0.00049962, 0.00049883, ..., 0.00050049,\n",
       "         0.00049939, 0.00049897],\n",
       "        [0.00050047, 0.00049992, 0.00050046, ..., 0.00049977,\n",
       "         0.00050067, 0.00050086],\n",
       "        ...,\n",
       "        [0.00049919, 0.0005004 , 0.0004999 , ..., 0.00050048,\n",
       "         0.00049994, 0.00049946],\n",
       "        [0.00049969, 0.00049927, 0.00049965, ..., 0.00049848,\n",
       "         0.00050036, 0.00050024],\n",
       "        [0.00050047, 0.00049992, 0.00050046, ..., 0.00049977,\n",
       "         0.00050067, 0.00050086]],\n",
       "\n",
       "       [[0.00050121, 0.00049981, 0.00049995, ..., 0.00049996,\n",
       "         0.00050034, 0.00050111],\n",
       "        [0.00049977, 0.00049966, 0.00049989, ..., 0.0004998 ,\n",
       "         0.00050051, 0.00049923],\n",
       "        [0.00049943, 0.00049977, 0.00049997, ..., 0.00049976,\n",
       "         0.00049987, 0.00049954],\n",
       "        ...,\n",
       "        [0.00049984, 0.00049986, 0.00050054, ..., 0.00049985,\n",
       "         0.00049997, 0.00049983],\n",
       "        [0.00050038, 0.00049877, 0.0004998 , ..., 0.0004986 ,\n",
       "         0.00049974, 0.00050007],\n",
       "        [0.0004992 , 0.00050008, 0.00050067, ..., 0.00049957,\n",
       "         0.00049964, 0.00050043]],\n",
       "\n",
       "       [[0.00050059, 0.00049874, 0.00049947, ..., 0.00049988,\n",
       "         0.00050045, 0.00049989],\n",
       "        [0.00049993, 0.00049949, 0.0004999 , ..., 0.00049951,\n",
       "         0.00050018, 0.00049991],\n",
       "        [0.00049943, 0.00050053, 0.00050058, ..., 0.00049993,\n",
       "         0.0004998 , 0.00050108],\n",
       "        ...,\n",
       "        [0.00050071, 0.00049952, 0.000499  , ..., 0.00050043,\n",
       "         0.00050093, 0.0004993 ],\n",
       "        [0.00050004, 0.00050061, 0.00049956, ..., 0.00050016,\n",
       "         0.00049994, 0.00049976],\n",
       "        [0.00049984, 0.0004998 , 0.00050001, ..., 0.00050065,\n",
       "         0.00049997, 0.00049931]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.00049999, 0.00050009, 0.00049968, ..., 0.00050019,\n",
       "         0.00049991, 0.00049971],\n",
       "        [0.00050064, 0.00050064, 0.00049991, ..., 0.00050013,\n",
       "         0.00050049, 0.00050086],\n",
       "        [0.00050064, 0.00049927, 0.00049853, ..., 0.0004994 ,\n",
       "         0.00049908, 0.00049931],\n",
       "        ...,\n",
       "        [0.00050063, 0.00050056, 0.00050066, ..., 0.00049986,\n",
       "         0.00050003, 0.00049995],\n",
       "        [0.00050015, 0.00049919, 0.00050015, ..., 0.0004994 ,\n",
       "         0.00050037, 0.00049999],\n",
       "        [0.0004992 , 0.0005005 , 0.00049966, ..., 0.00049972,\n",
       "         0.00049933, 0.00049983]],\n",
       "\n",
       "       [[0.00049983, 0.00049984, 0.00050048, ..., 0.00049987,\n",
       "         0.0004991 , 0.00049956],\n",
       "        [0.00050102, 0.00049966, 0.00049971, ..., 0.00050062,\n",
       "         0.00050107, 0.00050049],\n",
       "        [0.0005005 , 0.00049966, 0.00049906, ..., 0.00050046,\n",
       "         0.00050031, 0.00049909],\n",
       "        ...,\n",
       "        [0.00049996, 0.00049984, 0.00049957, ..., 0.00050115,\n",
       "         0.00050043, 0.00049915],\n",
       "        [0.00050083, 0.00050004, 0.0004988 , ..., 0.00049959,\n",
       "         0.0005001 , 0.00049922],\n",
       "        [0.00050071, 0.00049978, 0.00049959, ..., 0.0005002 ,\n",
       "         0.00050108, 0.00050062]],\n",
       "\n",
       "       [[0.00049973, 0.00050003, 0.00050003, ..., 0.00049965,\n",
       "         0.00050076, 0.00050024],\n",
       "        [0.00050104, 0.00050032, 0.00049963, ..., 0.0004999 ,\n",
       "         0.00049963, 0.00049988],\n",
       "        [0.00049956, 0.00049974, 0.00049957, ..., 0.00049963,\n",
       "         0.00050048, 0.00050007],\n",
       "        ...,\n",
       "        [0.00050034, 0.00049944, 0.00049987, ..., 0.00049886,\n",
       "         0.00049938, 0.00049978],\n",
       "        [0.00049931, 0.00049938, 0.00050005, ..., 0.00050009,\n",
       "         0.00049965, 0.0004991 ],\n",
       "        [0.00050024, 0.00049998, 0.00049961, ..., 0.00050075,\n",
       "         0.00050008, 0.00049981]]], dtype=float32)>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_tensor = tf.random.uniform((N, T),maxval = V, dtype = tf.int32)\n",
    "decoder_input_tensor = tf.random.uniform((N, Ty),maxval = V, dtype = tf.int32)\n",
    "final_model(encoder_input_tensor, decoder_input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
